{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataPipeline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tokenizers"
      ],
      "metadata": {
        "id": "zmsMvwhMC5cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "lnJ9CdIpC78n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tokenizers\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "T5y22NMHB6Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'roberta-base'\n",
        "MAX_LEN = 50\n",
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "roberta_model = TFRobertaModel.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "iiA_q6wxDIRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function takes text as an input and provides prediction either negative,neutral or positive\n",
        "def sentiment_analyzer():\n",
        "    t=input('Please Enter Text: ')\n",
        "    def decontracted(phrase):\n",
        "        # specific\n",
        "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "        # general\n",
        "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "        return phrase\n",
        "    def preprocess(txt):\n",
        "        '''This function pre processes given text data'''\n",
        "        txt=txt.replace('&amp;','and')\n",
        "        txt=re.sub(r'http\\S+', '', txt)\n",
        "        txt=decontracted(txt)\n",
        "        txt=\" \".join(filter(lambda x:x[0]!='@', txt.split()))\n",
        "        txt=\" \".join(filter(lambda x:x[0]!='#', txt.split()))\n",
        "        txt=\" \".join(filter(lambda x:x[-4:]!='.com', txt.split()))\n",
        "        txt=re.sub('[^A-Za-z ]+','', txt)\n",
        "\n",
        "        return txt.lower().strip()\n",
        "    t=decontracted(t)\n",
        "    t=preprocess(t)\n",
        "    def tokenization_(dataset):\n",
        "        allowed=48\n",
        "        dataset_tokens=[]\n",
        "        dataset_mask=[]\n",
        "        dataset_segment=[]\n",
        "        for i in range(len(dataset)):\n",
        "            mask=[]\n",
        "            tokenized=tokenizer.tokenize(dataset[i])\n",
        "            while len(tokenized)<allowed:\n",
        "                tokenized.append('[PAD]')\n",
        "            while len(tokenized)>allowed:\n",
        "                del tokenized[len(tokenized)-1]\n",
        "            if len(tokenized)==allowed:\n",
        "                tokens=['[CLS]',*tokenized,'[SEP]']\n",
        "\n",
        "            for j in tokens:\n",
        "                if j=='[PAD]':\n",
        "                    mask.append(0)\n",
        "                elif j!='[PAD]':\n",
        "                    mask.append(1)\n",
        "\n",
        "            segment=np.array([0]*50)\n",
        "            token_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "            dataset_tokens.append(token_ids)\n",
        "            dataset_mask.append(mask)\n",
        "            dataset_segment.append(segment)\n",
        "        return np.array(dataset_tokens),np.array(dataset_mask),np.array(dataset_segment)\n",
        "    x,y,z=tokenization_([t])\n",
        "    X={\n",
        "        'input_word_ids': x,\n",
        "        'input_mask': y,\n",
        "        'input_type_ids': z\n",
        "        }\n",
        "    input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "    input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n",
        "    input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n",
        "\n",
        "        \n",
        "    x = roberta_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n",
        "\n",
        "    x = x[0]\n",
        "\n",
        "    x = tf.keras.layers.Dropout(0.1)(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    model.load_weights('/content/drive/MyDrive/roberta.h5')\n",
        "    pred=model.predict(X)\n",
        "    i=np.argmax(pred)\n",
        "    if i==0:\n",
        "        return 'Given Sentence is Negative'\n",
        "    elif i==1:\n",
        "        return 'Given Sentence is Neutral'\n",
        "    elif i==2:\n",
        "        return 'Given Sentence is Positive'\n"
      ],
      "metadata": {
        "id": "UuFnt2SqCnNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "fR0UCMTpEsyJ",
        "outputId": "34c1f14d-f83b-479c-dabb-055a1e16aa1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please Enter Text: i am sad\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Given Sentence is Negative'"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "7LMOymE1Pb1J",
        "outputId": "d69c6220-8f21-46e3-8968-5efc6f6c907b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please Enter Text: i am happy to hear that\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Given Sentence is Positive'"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "QHg1U3KyPmGO",
        "outputId": "8ffc2fbf-baaf-4589-e38f-49fa654e04ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please Enter Text: he is in pain\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Given Sentence is Negative'"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "rzGOTVDVPy0i",
        "outputId": "2e6f71bc-e2fa-4ad4-d7ce-2933541b0be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please Enter Text: i indirectly liked it\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Given Sentence is Positive'"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "-5bCXe-TP4es",
        "outputId": "f463c2ce-60d5-4965-d480-0ea28851d56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please Enter Text: American biographical drama film directed by Gabriele Muccino and starring Will Smith as Chris Gardner, a homeless salesman. Smith's son Jaden Smith co-stars, making his film debut as Gardner's son, Christopher Jr. The screenplay by Steven Conrad is based on the best-selling 2006 memoir of the same name written by Gardner with Quincy Troupe.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Given Sentence is Positive'"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "34a9GxgoQPVC",
        "outputId": "27f72daf-f016-41b9-9949-4a0bcfbffa3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please Enter Text: While Gardner is trying to sell one of the scanners, he meets Jay Twistle\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Given Sentence is Positive'"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "CfwDafQ-QYAG",
        "outputId": "c2c699cd-27f7-4ccc-c72d-2edd8394b21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please Enter Text: Gardner's unpaid internship does not please Linda, who eventually leaves for New York,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Given Sentence is Negative'"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this function takes text and actual class label from user and gives accuracy score with model's prediction about text \n",
        "def Performance_sentiment_analyzer():\n",
        "    t=input('Please Enter Text: ')\n",
        "    target=input('Please mention sentiment\\n 0:Negative\\n 1:Neutral\\n 2:Positive: ')\n",
        "    while target.isdigit()==False:\n",
        "        target=input('Please mention sentiment\\n 0:Negative\\n 1:Neutral\\n 2:Positive: ')\n",
        "    def decontracted(phrase):\n",
        "        # specific\n",
        "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "        # general\n",
        "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "        return phrase\n",
        "    def preprocess(txt):\n",
        "        '''This function pre processes given text data'''\n",
        "        txt=txt.replace('&amp;','and')\n",
        "        txt=re.sub(r'http\\S+', '', txt)\n",
        "        txt=decontracted(txt)\n",
        "        txt=\" \".join(filter(lambda x:x[0]!='@', txt.split()))\n",
        "        txt=\" \".join(filter(lambda x:x[0]!='#', txt.split()))\n",
        "        txt=\" \".join(filter(lambda x:x[-4:]!='.com', txt.split()))\n",
        "        txt=re.sub('[^A-Za-z ]+','', txt)\n",
        "\n",
        "        return txt.lower().strip()\n",
        "    t=decontracted(t)\n",
        "    t=preprocess(t)\n",
        "    def tokenization_(dataset):\n",
        "        allowed=48\n",
        "        dataset_tokens=[]\n",
        "        dataset_mask=[]\n",
        "        dataset_segment=[]\n",
        "        for i in range(len(dataset)):\n",
        "            mask=[]\n",
        "            tokenized=tokenizer.tokenize(dataset[i])\n",
        "            while len(tokenized)<allowed:\n",
        "                tokenized.append('[PAD]')\n",
        "            while len(tokenized)>allowed:\n",
        "                del tokenized[len(tokenized)-1]\n",
        "            if len(tokenized)==allowed:\n",
        "                tokens=['[CLS]',*tokenized,'[SEP]']\n",
        "\n",
        "            for j in tokens:\n",
        "                if j=='[PAD]':\n",
        "                    mask.append(0)\n",
        "                elif j!='[PAD]':\n",
        "                    mask.append(1)\n",
        "\n",
        "            segment=np.array([0]*50)\n",
        "            token_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "            dataset_tokens.append(token_ids)\n",
        "            dataset_mask.append(mask)\n",
        "            dataset_segment.append(segment)\n",
        "        return np.array(dataset_tokens),np.array(dataset_mask),np.array(dataset_segment)\n",
        "    x,y,z=tokenization_([t])\n",
        "    X={\n",
        "        'input_word_ids': x,\n",
        "        'input_mask': y,\n",
        "        'input_type_ids': z\n",
        "        }\n",
        "    input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "    input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n",
        "    input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n",
        "\n",
        "        \n",
        "    x = roberta_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n",
        "\n",
        "    x = x[0]\n",
        "\n",
        "    x = tf.keras.layers.Dropout(0.1)(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "        #x=RandomFourierFeatures(output_dim=4090, scale=10.0, kernel_initializer=\"gaussian\")(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    model.load_weights('/content/drive/MyDrive/roberta.h5')\n",
        "    pred=model.predict(X)\n",
        "    pred=np.argmax(pred)\n",
        "    if int(target)==int(pred):\n",
        "        acc=1\n",
        "    elif int(target)!=int(pred):\n",
        "        acc=0\n",
        "    print('Accuracy score is {}'.format(acc))"
      ],
      "metadata": {
        "id": "-JG-cBIGJ-da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Performance_sentiment_analyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDF8zrfRKX1W",
        "outputId": "be0aaa65-22e3-4404-c3f4-4ae6541616f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please Enter Text: Gardner's unpaid internship does not please Linda, who eventually leaves for New York,\n",
            "Please mention sentiment\n",
            " 0:Negative\n",
            " 1:Neutral\n",
            " 2:Positive: 0\n",
            "Accuracy score is 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Performance_sentiment_analyzer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRbKxZluQqqD",
        "outputId": "d994c183-e7c0-406b-de9a-8d5fb3d0b3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please Enter Text: Gardner's unpaid internship does not please Linda, who eventually leaves for New York,\n",
            "Please mention sentiment\n",
            " 0:Negative\n",
            " 1:Neutral\n",
            " 2:Positive: \n",
            "Please mention sentiment\n",
            " 0:Negative\n",
            " 1:Neutral\n",
            " 2:Positive: 2\n",
            "Accuracy score is 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R-s_ryPPQx7M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}